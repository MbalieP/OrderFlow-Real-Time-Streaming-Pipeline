#  OrderFlow: Real-Time E-Commerce Order Tracking Pipeline

A production-ready real-time data pipeline using Apache Kafka, Apache Spark Structured Streaming, and MySQL for tracking e-commerce orders.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.7%2B-blue)
![Spark](https://img.shields.io/badge/Spark-3.0%2B-orange)

## ğŸ“‘ Table of Contents
- [Overview](#-overview)
- [Tech Stack](#-tech-stack)
- [Features](#-features)
- [System Architecture](#-system-architecture)
- [Quick Start](#-quick-start)
- [Configuration](#-configuration)
- [Message Schema](#-message-schema)
- [Project Structure](#-project-structure)
- [Usage Examples](#-usage-examples)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [About](#-about)

## ğŸ“¦ Overview

This project demonstrates a production-ready real-time data pipeline for e-commerce order processing. The pipeline simulates a complete order lifecycle from generation to persistent storage.

### Pipeline Flow
1. **ğŸ“¤ Producer Layer**: Generates simulated order data and publishes to Kafka
2. **ğŸ“¬ Message Queue**: Apache Kafka handles message buffering and distribution
3. **âš¡ Processing Layer**: Spark Structured Streaming processes messages in micro-batches
4. **ğŸ’¾ Storage Layer**: Processed orders are stored in MySQL for analytics

## ğŸ›  Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| Producer | Python + kafka-python | Order data generation and publishing |
| Message Queue | Apache Kafka | Real-time message streaming |
| Stream Processing | Apache Spark Structured Streaming | Micro-batch processing |
| Database | MySQL | Persistent storage for analytics |
| Language | Python 3.7+ | Primary development language |

## ğŸ’¡ Features

### ğŸ¯ Producer Features
- âœ… **Robust Error Handling**: Automatic retry logic with configurable connection retries
- âœ… **Graceful Shutdown**: Handles SIGINT/SIGTERM signals cleanly with message flushing
- âœ… **Comprehensive Logging**: Dual output to file (`kafka_producer.log`) and console
- âœ… **Message Delivery Tracking**: Success/error callbacks for each message
- âœ… **Environment Configuration**: Full support for environment variables
- âœ… **Production Ready**: Idempotence, compression (GZIP), and ACKs configuration
- âœ… **Type Safety**: Complete type hints for better code quality

### âš¡ Pipeline Features
- ğŸ”„ **Real-time Processing**: Micro-batch stream processing with Spark Structured Streaming
- ğŸ“Š **Schema Enforcement**: JSON parsing with strict schema validation
- ğŸ’¾ **Batch Optimization**: Efficient batch-wise writing to MySQL
- ğŸ”Œ **Extensible**: Easy integration with Apache Airflow, dashboards, and monitoring tools
- ğŸ›¡ï¸ **Fault Tolerant**: Checkpointing and error recovery mechanisms

## ğŸ— System Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Kafka Producer â”‚ â”€â”€> â”‚ Apache Kafka â”‚ â”€â”€> â”‚ Spark Structured â”‚ â”€â”€> â”‚ MySQL â”‚
â”‚ (Python) â”‚ â”‚ (Message Bus) â”‚ â”‚ Streaming â”‚ â”‚ (Storage) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

text

### ğŸ“Š Data Flow Sequence
1. Order events generated by producer
2. Events published to Kafka topic
3. Spark consumes and processes events in micro-batches
4. Processed data written to MySQL for analytics

## ğŸš€ Quick Start

### Prerequisites
- Python 3.7 or higher
- Apache Kafka (running on localhost:9092)
- Apache Spark 3.0+ with PySpark
- MySQL 8.0+ (running on localhost:3306)

### Installation Steps

1. **Clone the Repository**
   ```bash
   git clone <repository-url>
   cd orderflow-ecommerce-pipeline
Install Python Dependencies

bash
pip install -r requirements.txt
Or install manually:

bash
pip install kafka-python pyspark mysql-connector-python
Set Up Kafka
Ensure Kafka is running on localhost:9092. Create the topic:

bash
kafka-topics.sh --create \
  --topic orders \
  --bootstrap-server localhost:9092 \
  --partitions 3 \
  --replication-factor 1
Set Up MySQL
Create the database and table:

sql
CREATE DATABASE ecommerce;
USE ecommerce;

CREATE TABLE orders (
    customer_id INT,
    order_id VARCHAR(50),
    product_name VARCHAR(255),
    product_category VARCHAR(100),
    unit_price DECIMAL(10, 2),
    quantity INT,
    total_price DECIMAL(10, 2),
    order_timestamp DATETIME,
    PRIMARY KEY (order_id),
    INDEX idx_customer (customer_id),
    INDEX idx_timestamp (order_timestamp)
);
Run the Producer

bash
python kafka_order_producer.py
Run the Consumer (in another terminal)

bash
python spark_to_mysql.py
âš™ï¸ Configuration
Environment Variables
All components support configuration via environment variables. Copy env.example to .env and customize:

bash
cp env.example .env
Producer Configuration
Variable	Default	Description
KAFKA_BOOTSTRAP_SERVERS	localhost:9092	Kafka broker address
KAFKA_TOPIC	orders	Kafka topic name
PRODUCTION_INTERVAL	2	Seconds between order generation
STARTING_ORDER_ID	1	Initial order number
Consumer Configuration
Variable	Default	Description
KAFKA_BOOTSTRAP_SERVERS	localhost:9092	Kafka broker address
KAFKA_TOPIC	orders	Kafka topic name
MYSQL_HOST	localhost	MySQL host
MYSQL_PORT	3306	MySQL port
MYSQL_DATABASE	ecommerce	Database name
MYSQL_TABLE	orders	Table name
MYSQL_USER	root	Database user
MYSQL_PASSWORD	(empty)	Database password
Example Configuration
bash
# Set environment variables
export KAFKA_BOOTSTRAP_SERVERS='kafka-server:9092'
export PRODUCTION_INTERVAL='1.5'
export STARTING_ORDER_ID='1000'

# Run producer
python kafka_order_producer.py
ğŸ“‹ Message Schema
Sample Message
json
{
  "customer_id": 87654,
  "order_id": "ORDER-000001",
  "product_name": "Yoga Mat",
  "product_category": "Sports & Outdoors",
  "unit_price": 45.99,
  "quantity": 2,
  "total_price": 91.98,
  "order_timestamp": "2025-01-15T14:30:45"
}
Schema Definition
Field Name	Data Type	Description	Constraints
customer_id	Integer	Unique customer identifier	50000-99999
order_id	String	Sequential order ID	Format: ORDER-XXXXXX
product_name	String	Name of the product	From product catalog
product_category	String	Product category	Home & Garden, Sports, etc.
unit_price	Decimal	Price per unit	50.00 - 999.99
quantity	Integer	Number of items ordered	1-5
total_price	Decimal	Total order value	unit_price Ã— quantity
order_timestamp	String	ISO format timestamp	YYYY-MM-DDTHH:MM:SS
ğŸ“ Project Structure
text
orderflow-ecommerce-pipeline/
â”‚
â”œâ”€â”€ ğŸ“„ kafka_order_producer.py      # Main producer script
â”œâ”€â”€ ğŸ“„ spark_to_mysql.py            # Consumer - writes to MySQL
â”œâ”€â”€ ğŸ“„ spark_order_consumer.py      # Consumer - console output (debug)
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ğŸ“„ env.example                  # Environment variables template
â”œâ”€â”€ ğŸ“„ LICENSE                      # MIT License
â”œâ”€â”€ ğŸ“„ README.md                    # This file
â”‚
â””â”€â”€ ğŸ“Š Generated Files/
    â””â”€â”€ kafka_producer.log          # Producer log file (auto-generated)
File Descriptions
File	Purpose	Lines	Status
kafka_order_producer.py	Generates and publishes order messages to Kafka	~200	âœ… Production-ready
spark_to_mysql.py	Consumes Kafka stream and writes to MySQL	~115	âœ… Robust
spark_order_consumer.py	Console output for testing/debugging	~70	ğŸ”§ Development tool
requirements.txt	Python package dependencies	-	ğŸ“¦ Required
env.example	Configuration template	-	ğŸ“‹ Template
ğŸ’» Usage Examples
Basic Usage
Start Producer:

bash
python kafka_order_producer.py
Start Consumer:

bash
python spark_to_mysql.py
Test/Debug Consumer:

bash
python spark_order_consumer.py
Advanced Usage
Custom Producer Configuration:

bash
export KAFKA_BOOTSTRAP_SERVERS='remote-kafka:9092'
export PRODUCTION_INTERVAL='0.5'  # Faster production
export STARTING_ORDER_ID='5000'
python kafka_order_producer.py
Custom Consumer Configuration:

bash
export MYSQL_HOST='db-server'
export MYSQL_PASSWORD='secure_password'
export MYSQL_DATABASE='production_orders'
python spark_to_mysql.py
Monitoring
View Producer Logs:

bash
tail -f kafka_producer.log
Check Kafka Messages:

bash
kafka-console-consumer.sh \
  --topic orders \
  --from-beginning \
  --bootstrap-server localhost:9092
Query MySQL Orders:

sql
SELECT * FROM orders ORDER BY order_timestamp DESC LIMIT 10;
